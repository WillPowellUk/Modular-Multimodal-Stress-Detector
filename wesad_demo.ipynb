{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Modular Multimodal Data Fusion ML Pipeline for stress detection for the WESAD Database\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started:\n",
    "First, download necessary packages, if you are using a venv such as Conda, activate this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Installation\n",
    "If you are on Linux, run this cell to download and extract the WESAD dataset automatically, otherwise download manually [here](https://uni-siegen.sciebo.de/s/HGdUkoNlW1Ub0Gx/download) and unzip the `WESAD` file into the `wesad` directory i.e. `wesad/WESAD/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd src/wesad && bash download_database.sh\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This will automatically extract the biosensor data from the WESAD directory into several merged files in `.pkl` format.\n",
    "\n",
    "This will take around 10 minutes depending on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.wesad.data_preprocessing import WESADDataPreprocessor\n",
    "\n",
    "preprocessor = WESADDataPreprocessor('src/wesad/WESAD/')\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Preprocessing Steps\n",
    "We will preprocess each signal with their respective preprocessing steps:\n",
    "\n",
    "### Chest Signals\n",
    "\n",
    "#### ECG\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.7 Hz and 3.7 Hz.\n",
    "\n",
    "#### EMG\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth lowpass filter of order 3 with cutoff frequency 0.5 Hz.\n",
    "\n",
    "#### EDA\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth lowpass filter of order 2 with cutoff frequency 5 Hz.\n",
    "\n",
    "#### TEMP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "\n",
    "#### RESP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.1 Hz and 0.35 Hz.\n",
    "\n",
    "#### ACC\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 31 and order 5.\n",
    "\n",
    "### Wrist Signals\n",
    "\n",
    "#### BVP\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.7 Hz and 3.7 Hz.\n",
    "\n",
    "#### TEMP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "\n",
    "#### ACC\n",
    "- **Filtering**: Finite Impulse Response (FIR) filter with a length of 64 with a cut-off frequency of 0.4 Hz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config_files\n",
    "CHEST_CONFIG = 'config_files/dataset/wesad_chest_configuration.json'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.preprocessing import SignalPreprocessor\n",
    "\n",
    "# preprocess the chest data\n",
    "signal_preprocessor = SignalPreprocessor('src/wesad/WESAD/raw/merged_chest.pkl', 'src/wesad/WESAD/cleaned/chest_preprocessed.pkl', CHEST_CONFIG)\n",
    "signal_preprocessor.preprocess_signals()\n",
    "\n",
    "# preprocess the wrist data\n",
    "signal_preprocessor = SignalPreprocessor('src/wesad/WESAD/raw/merged_wrist.pkl', 'src/wesad/WESAD/cleaned/wrist_preprocessed.pkl', WRIST_CONFIG, wrist=True)\n",
    "signal_preprocessor.preprocess_signals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>ecg</th>\n",
       "      <th>emg</th>\n",
       "      <th>eda</th>\n",
       "      <th>temp</th>\n",
       "      <th>resp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214583</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8914</td>\n",
       "      <td>-0.1102</td>\n",
       "      <td>-0.2576</td>\n",
       "      <td>0.030945</td>\n",
       "      <td>-0.003708</td>\n",
       "      <td>5.710983</td>\n",
       "      <td>29.083618</td>\n",
       "      <td>1.191711</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214584</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8926</td>\n",
       "      <td>-0.1086</td>\n",
       "      <td>-0.2544</td>\n",
       "      <td>0.033646</td>\n",
       "      <td>-0.014145</td>\n",
       "      <td>5.719376</td>\n",
       "      <td>29.122437</td>\n",
       "      <td>1.139832</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214585</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8930</td>\n",
       "      <td>-0.1094</td>\n",
       "      <td>-0.2580</td>\n",
       "      <td>0.033005</td>\n",
       "      <td>0.010208</td>\n",
       "      <td>5.706406</td>\n",
       "      <td>29.115234</td>\n",
       "      <td>1.141357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214586</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8934</td>\n",
       "      <td>-0.1082</td>\n",
       "      <td>-0.2538</td>\n",
       "      <td>0.031815</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>5.712509</td>\n",
       "      <td>29.126709</td>\n",
       "      <td>1.155090</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214587</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8930</td>\n",
       "      <td>-0.1096</td>\n",
       "      <td>-0.2570</td>\n",
       "      <td>0.030350</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>5.727005</td>\n",
       "      <td>29.100861</td>\n",
       "      <td>1.133728</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59628259</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.6494</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>-0.6788</td>\n",
       "      <td>0.175003</td>\n",
       "      <td>0.033646</td>\n",
       "      <td>7.341385</td>\n",
       "      <td>34.592560</td>\n",
       "      <td>-2.023315</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59628260</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.6486</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>-0.6774</td>\n",
       "      <td>0.188828</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>7.352066</td>\n",
       "      <td>34.603302</td>\n",
       "      <td>-2.000427</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59628261</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.6508</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>-0.6760</td>\n",
       "      <td>0.201965</td>\n",
       "      <td>-0.007874</td>\n",
       "      <td>7.345200</td>\n",
       "      <td>34.584900</td>\n",
       "      <td>-1.994324</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59628262</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>-0.6762</td>\n",
       "      <td>0.212631</td>\n",
       "      <td>-0.011856</td>\n",
       "      <td>7.351303</td>\n",
       "      <td>34.543488</td>\n",
       "      <td>-2.003479</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59628263</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.6498</td>\n",
       "      <td>0.0846</td>\n",
       "      <td>-0.6756</td>\n",
       "      <td>0.220413</td>\n",
       "      <td>-0.008469</td>\n",
       "      <td>7.342148</td>\n",
       "      <td>34.584900</td>\n",
       "      <td>-2.030945</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23206321 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sid    acc1    acc2    acc3       ecg       emg       eda  \\\n",
       "214583     2.0  0.8914 -0.1102 -0.2576  0.030945 -0.003708  5.710983   \n",
       "214584     2.0  0.8926 -0.1086 -0.2544  0.033646 -0.014145  5.719376   \n",
       "214585     2.0  0.8930 -0.1094 -0.2580  0.033005  0.010208  5.706406   \n",
       "214586     2.0  0.8934 -0.1082 -0.2538  0.031815  0.012634  5.712509   \n",
       "214587     2.0  0.8930 -0.1096 -0.2570  0.030350  0.002060  5.727005   \n",
       "...        ...     ...     ...     ...       ...       ...       ...   \n",
       "59628259  17.0  0.6494  0.0898 -0.6788  0.175003  0.033646  7.341385   \n",
       "59628260  17.0  0.6486  0.0894 -0.6774  0.188828 -0.001099  7.352066   \n",
       "59628261  17.0  0.6508  0.0894 -0.6760  0.201965 -0.007874  7.345200   \n",
       "59628262  17.0  0.6500  0.0862 -0.6762  0.212631 -0.011856  7.351303   \n",
       "59628263  17.0  0.6498  0.0846 -0.6756  0.220413 -0.008469  7.342148   \n",
       "\n",
       "               temp      resp  label  \n",
       "214583    29.083618  1.191711    1.0  \n",
       "214584    29.122437  1.139832    1.0  \n",
       "214585    29.115234  1.141357    1.0  \n",
       "214586    29.126709  1.155090    1.0  \n",
       "214587    29.100861  1.133728    1.0  \n",
       "...             ...       ...    ...  \n",
       "59628259  34.592560 -2.023315    2.0  \n",
       "59628260  34.603302 -2.000427    2.0  \n",
       "59628261  34.584900 -1.994324    2.0  \n",
       "59628262  34.543488 -2.003479    2.0  \n",
       "59628263  34.584900 -2.030945    2.0  \n",
       "\n",
       "[23206321 rows x 10 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "file = 'src/wesad/WESAD/raw/merged_chest.pkl'\n",
    "with open(file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation and Splitting\n",
    "Data augmentation will take the form of a sliding window. Once the data is augmented, each sample will then be split into smaller segments if it is not used in the autoregression case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import DataAugmenter\n",
    "\n",
    "AUTOREGRESSIVE = True\n",
    "\n",
    "if AUTOREGRESSIVE:\n",
    "    WINDOW_LENGTH = 10\n",
    "    SLIDING_LENGTH = 2 # this will create 5 segments per 10 seconds since 10/2 = 5 with 4:1 ratio of synthetic to real samples\n",
    "    SPLIT_LENGTH = WINDOW_LENGTH # this will not sub-split the data\n",
    "else: \n",
    "    WINDOW_LENGTH = 60\n",
    "    SLIDING_LENGTH = 5 # this will create 12 segments per minute since 60/5 = 12 with 11:1 ratio of synthetic to real samples\n",
    "    SPLIT_LENGTH = 10 # this will sub-split each 60 second segments into 6 x 10 second segments\n",
    "    WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "\n",
    "wrist_augmenter = DataAugmenter('src/wesad/WESAD/cleaned/wrist_preprocessed.pkl', WRIST_CONFIG) \n",
    "batches = wrist_augmenter.augment_data(WINDOW_LENGTH, SLIDING_LENGTH)\n",
    "wrist_splitted_segments = wrist_augmenter.split_segments(batches, WINDOW_LENGTH//SPLIT_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning\n",
    "\n",
    "The manual feature extraction derives features in the time, frequency and non-linear domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.feature_extraction import ManualFE\n",
    "\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "\n",
    "manual_fe = ManualFE(wrist_splitted_segments, WRIST_FE, WRIST_CONFIG)\n",
    "manual_fe.extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LOSOCV Datasets\n",
    "\n",
    "Now, using the preprocessed `.pkl` files we will make it into a dataloader with LOSOCV (Leave one subject out cross validation). The data augmented samples will be used in the training set but ignored in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVDataLoader\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "losocv_loader = LOSOCVDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "DATASETS_PATH = losocv_loader.prepare_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate Models with LOSOCV\n",
    "\n",
    "Now we can use the prepared datasets and form dataloaders which will then be used to perform LOSOCV on the models. Using the config file we can set the models that we want to test and their corresponding hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVDataLoader\n",
    "from src.ml_pipeline.train import TraditionalMLTrainer\n",
    "from src.utils import save_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load tradtional model config\n",
    "losocv_loader = LOSOCVDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders = losocv_loader.get_data_loaders(DATASETS_PATH)\n",
    "TRADTIONAL_ML_CONFIG = 'config_files/model_training/traditional/traditional_models.json'\n",
    "\n",
    "results = []\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'\\nFold {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = TraditionalMLTrainer(TRADTIONAL_ML_CONFIG, train_loader, val_loader)\n",
    "\n",
    "    # trained_models = trainer.tune_hyperparameters(n_jobs=4, cv=None, verbose=2)\n",
    "    trained_models = trainer.train()\n",
    "\n",
    "    result = trainer.validate(trained_models)\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "# save the results to pkl\n",
    "save_var(results, 'src/wesad/WESAD/results/traditional_models/wrist_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "results = load_var('src/wesad/WESAD/results/traditional_models/wrist_results.pkl')\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Network Alongside Manual Feature Extraction\n",
    "This network employs self-attention networks for intra-modality feature extraction before applying late fusion between modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LOSOCV Datasets on a Per Sensor Basis\n",
    "\n",
    "Now, using the preprocessed `.pkl` files we will make it into a dataloader with LOSOCV (Leave one subject out cross validation) on a per sensor basis. The data augmented samples will be used in the training set but ignored in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "losocv_loader = LOSOCVSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "DATASETS_PATH = losocv_loader.prepare_datasets(f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate Models with LOSOCV\n",
    "\n",
    "Now we can use the prepared datasets and form dataloaders which will then be used to perform LOSOCV on the models. Using the config file we can set the models that we want to test and their corresponding hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config_files/dataset/wesad_wrist_configuration.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m dataloader_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: get_values(SAN_MODEL_CONFIG, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# 'num_workers': 4\u001b[39;00m\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Load Dataloaders for LOSOCV\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m losocv_loader \u001b[38;5;241m=\u001b[39m \u001b[43mLOSOCVSensorDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWRIST_FE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWRIST_CONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataloader_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m dataloaders, input_dims \u001b[38;5;241m=\u001b[39m losocv_loader\u001b[38;5;241m.\u001b[39mget_data_loaders(DATASETS_PATH)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Load Model Parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/Imperial FYP/ Modular-Multimodal-Stress-Detector/src/ml_pipeline/data_loader/losocv_sensor_data_loader.py:17\u001b[0m, in \u001b[0;36mLOSOCVSensorDataLoader.__init__\u001b[0;34m(self, features_path, config_path, **params)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_path \u001b[38;5;241m=\u001b[39m features_path\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_path \u001b[38;5;241m=\u001b[39m config_path\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_sensors\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mget_active_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_active_key(config_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_active_key(config_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     20\u001b[0m }\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubjects \u001b[38;5;241m=\u001b[39m get_active_key(config_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubjects\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m params\n",
      "File \u001b[0;32m~/Dev/Imperial FYP/ Modular-Multimodal-Stress-Detector/src/ml_pipeline/utils/utils.py:39\u001b[0m, in \u001b[0;36mget_active_key\u001b[0;34m(config_path, key, recursive)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_active_key\u001b[39m(config_path, key, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     40\u001b[0m         config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recursive:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config_files/dataset/wesad_wrist_configuration.json'"
     ]
    }
   ],
   "source": [
    "from src.ml_pipeline.train import PyTorchTrainer\n",
    "from src.ml_pipeline.models.attention_models import ModularModalityFusionNet\n",
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "from src.ml_pipeline.utils import get_active_key, get_key, load_json, copy_json, get_values\n",
    "from src.utils import save_var\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/losocv_datasets.pkl'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "SAN_MODEL_CONFIG = 'config_files/model_training/deep/modular_modality_fusion_net_config.json'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': get_values(SAN_MODEL_CONFIG, 'batch_size'),\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load Dataloaders for LOSOCV\n",
    "losocv_loader = LOSOCVSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders, input_dims = losocv_loader.get_data_loaders(DATASETS_PATH)\n",
    "\n",
    "# Load Model Parameters\n",
    "model_config = load_json(SAN_MODEL_CONFIG)\n",
    "model_config = {\n",
    "    **model_config,\n",
    "    'input_dims': input_dims\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "results = []\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'\\nSubject: {subject_id}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    model = ModularModalityFusionNet(**model_config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PyTorchTrainer(model, train_loader, val_loader, SAN_MODEL_CONFIG, device)\n",
    "    trainer.save_path = trainer.save_path.format(fold=f'subject_{subject_id}')\n",
    "    if i == 0:\n",
    "        trainer.print_model_summary()\n",
    "    trained_model_ckpt = trainer.train()\n",
    "    print(f'Model checkpoint saved to: {trained_model_ckpt}\\n')\n",
    "\n",
    "    result = trainer.validate(trained_model_ckpt)\n",
    "    results.append(result)\n",
    "    break\n",
    "\n",
    "# save the results to pkl\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path = f'src/wesad/WESAD/results/san/wrist_results/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/{current_time}/generalized'\n",
    "save_var(results, f'{save_path}/results.pkl', 'Results')\n",
    "copy_json(SAN_MODEL_CONFIG, f'{save_path}/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# results_path = 'src/wesad/WESAD/results/san/wrist_results/60s_5s_10s/2024_06_20_14_55_01/.pkl'\n",
    "results_path = f'{save_path}/results.pkl'\n",
    "results = load_var(results_path)\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losocv_datasets = {2: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_2.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_2.hdf5'}, 3: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_3.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_3.hdf5'}, 4: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_4.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_4.hdf5'}, 5: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_5.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_5.hdf5'}, 6: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_6.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_6.hdf5'}, 7: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_7.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_7.hdf5'}, 8: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_8.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_8.hdf5'}, 9: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_9.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_9.hdf5'}, 10: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_10.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_10.hdf5'}, 11: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_11.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_11.hdf5'}, 13: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_13.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_13.hdf5'}, 14: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_14.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_14.hdf5'}, 15: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_15.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_15.hdf5'}, 16: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_16.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_16.hdf5'}, 17: {'train': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/train_17.hdf5', 'val': 'src/wesad/WESAD/datasets/10s_2s_10s/losocv/val_17.hdf5'}}\n",
    "\n",
    "# save the updated losocv_datasets to the same file\n",
    "with open('src/wesad/WESAD/datasets/10s_2s_10s/losocv_datasets.pkl', 'wb') as f:\n",
    "    pickle.dump(losocv_datasets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Cross Validation Dataloader for Personalization\n",
    "\n",
    "First we must prepare a new dataloader which exclusively contains one subject's personalized data that is used to cross validate the personalized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import PersonalSensorDataLoader\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "SUBJECT_ID = 2\n",
    "personal_loader = PersonalSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "DATASETS_PATH = personal_loader.prepare_datasets(f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s', SUBJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Personalization via Transfer Learning\n",
    "\n",
    "This will then be used to fine tune the model with the new multiheaded attention blocks architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.train import PyTorchTrainer\n",
    "from src.ml_pipeline.models.attention_models import PersonalizedModalityFusionNet, ModularModalityFusionNet\n",
    "from src.ml_pipeline.data_loader import PersonalSensorDataLoader\n",
    "from src.ml_pipeline.utils import get_active_key, load_json, copy_json, get_values\n",
    "from src.utils import save_var\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "SUBJECT_ID = 2\n",
    "\n",
    "DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/subject_{SUBJECT_ID}/personal_dataset.pkl'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "PERSONALIZED_SAN_MODEL_CONFIG = 'config_files/model_training/deep/personalized_modality_fusion_net_config.json'\n",
    "MFN_CKPT_PATH = 'src/wesad/WESAD/ckpts/san/wrist_manual_fe/60s_5s_10s/generalized/subject_2/checkpoint_5.pth'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': get_values(PERSONALIZED_SAN_MODEL_CONFIG, 'batch_size'),\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load Dataloaders for LOSOCV\n",
    "personal_loader = PersonalSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders, input_dims = personal_loader.get_data_loaders(DATASETS_PATH)\n",
    "\n",
    "# Load Model Parameters\n",
    "model_config = load_json(PERSONALIZED_SAN_MODEL_CONFIG)\n",
    "model_config = {\n",
    "    **model_config,\n",
    "    'input_dims': input_dims\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "results = []\n",
    "for i, loaders in enumerate(dataloaders):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']    \n",
    "    print(f'\\nFold: {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    model = PersonalizedModalityFusionNet(MFN_CKPT_PATH, ModularModalityFusionNet, **model_config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PyTorchTrainer(model, train_loader, val_loader, PERSONALIZED_SAN_MODEL_CONFIG, device)\n",
    "    trainer.save_path = trainer.save_path.format(fold=f'fold_{i}')\n",
    "    if i == 0:\n",
    "        trainer.print_model_summary()\n",
    "    trained_model_ckpt = trainer.train()\n",
    "    print(f'Model saved at: {trained_model_ckpt}')\n",
    "\n",
    "    result = trainer.validate(trained_model_ckpt)\n",
    "    results.append(result)\n",
    "\n",
    "# save the results to pkl\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path = f'src/wesad/WESAD/results/san/wrist_results/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/{current_time}/personalized'\n",
    "save_var(results, f'{save_path}/results.pkl', 'results')\n",
    "copy_json(PERSONALIZED_SAN_MODEL_CONFIG, f'{save_path}/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "results_path = f'src/wesad/WESAD/results/san/wrist_results/60s_5s_10s/2024_06_22_14_20_25/personalized/results.pkl'\n",
    "results = load_var(results_path)\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Cross-and Self-modal Attention (BCSA) \n",
    "\n",
    "This model now integrates cross attention into the self-attention network for inter-modality and intra-modality feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate Model with LOSOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.train import PyTorchTrainer\n",
    "from src.ml_pipeline.models.attention_models import ModularBCSA\n",
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "from src.ml_pipeline.utils import load_json, copy_json, get_values\n",
    "from src.utils import save_var\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will subsplit each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}_mini/wrist_manual_fe_{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}.hdf5'\n",
    "DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/losocv_datasets.pkl'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s_mini/wrist_features.hdf5'\n",
    "BCSA_MODEL_CONFIG = 'config_files/model_training/deep/bsca_config.json'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': get_values(BCSA_MODEL_CONFIG, 'batch_size'),\n",
    "    'shuffle': True,\n",
    "}\n",
    "\n",
    "# Load Dataloaders for LOSOCV\n",
    "losocv_loader = LOSOCVSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders, input_dims = losocv_loader.get_data_loaders(DATASETS_PATH)\n",
    "\n",
    "# Load Model Parameters\n",
    "model_config = load_json(BCSA_MODEL_CONFIG)\n",
    "model_config = {\n",
    "    **model_config,\n",
    "    'input_dims': input_dims\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "results = []\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'\\nSubject: {subject_id}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    model = ModularBCSA(**model_config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PyTorchTrainer(model, train_loader, val_loader, BCSA_MODEL_CONFIG, device)\n",
    "    trainer.save_path = trainer.save_path.format(fold=f'subject_{subject_id}')\n",
    "    \n",
    "    # if i == 0:\n",
    "    #     trainer.print_model_summary()\n",
    "\n",
    "    trained_model_ckpt = trainer.train()\n",
    "    print(f'Model checkpoint saved to: {trained_model_ckpt}\\n')\n",
    "\n",
    "    result = trainer.validate(trained_model_ckpt)\n",
    "    results.append(result)\n",
    "    break\n",
    "\n",
    "# save the results to pkl\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path = f'src/wesad/WESAD/results/bcsa/wrist_results/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/{current_time}/generalized'\n",
    "save_var(results, f'{save_path}/results.pkl', 'Results')\n",
    "copy_json(BCSA_MODEL_CONFIG, f'{save_path}/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# results_path = f'{save_path}/results.pkl'\n",
    "# results_path = 'src/wesad/WESAD/results/bcsa/wrist_results/60s_5s_10s/2024_06_21_15_50_27/generalized/results.pkl'\n",
    "results_path = 'src/wesad/WESAD/results/bcsa/wrist_results/60s_5s_10s/2024_06_27_17_23_40/generalized/results.pkl'\n",
    "results = load_var(results_path)\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular Autoregressive Co-Attention Network (MARCONet)  \n",
    "\n",
    "This model now integrates modularity to the Self and Cross Attention Network for inter-modality and intra-modality feature extraction using both early fusion of pairwise modalities and late fusion of the ensemble learning branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LOSOCV Datasets on a Per Sensor Basis\n",
    "\n",
    "Now, using the preprocessed `.pkl` files we will make it into a dataloader with LOSOCV (Leave one subject out cross validation) on a per sensor basis. Two dataloaders will be created. One dataloader will be used to train the non-sliding model on batched temporal slices of the same length as seq_length in the config file (this will also contain batched validation data to measure progress). The second dataloader will be used stictly for validation after transfer learning to the sliding co-attention KV cached model. It will used unbatched (i.e. one temporal slice at a time) features of the left out subject and will not be augmented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "\n",
    "BATCHED_WINDOW_LENGTH = 30\n",
    "BATCHED_SPLIT_LENGTH = int(BATCHED_WINDOW_LENGTH / 6) # this will sub-split the data 6 times each of 5 seconds\n",
    "BATCHED_SLIDING_LENGTH = BATCHED_SPLIT_LENGTH # this will create 6 samples per 30 seconds since 30/5 = 6 with 5:1 ratio of synthetic to real samples\n",
    "\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "BATCHED_WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{BATCHED_WINDOW_LENGTH}s_{BATCHED_SLIDING_LENGTH}s_{BATCHED_SPLIT_LENGTH}s/BATCHED_wrist_features.hdf5'\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "train_losocv_loader = LOSOCVSensorDataLoader(BATCHED_WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "BATCHED_DATASETS_PATH = train_losocv_loader.prepare_datasets(f'src/wesad/WESAD/datasets/wrist/all/{BATCHED_WINDOW_LENGTH}s_{BATCHED_SLIDING_LENGTH}s_{BATCHED_SPLIT_LENGTH}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "\n",
    "NON_BATCHED_WINDOW_LENGTH = 5\n",
    "NON_BATCHED_SLIDING_LENGTH = NON_BATCHED_WINDOW_LENGTH # this will create no overlap between segments i.e. no augmented / synthetic data.\n",
    "NON_BATCHED_SPLIT_LENGTH = NON_BATCHED_WINDOW_LENGTH # this will not sub-split the data\n",
    "\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "NON_BATCHED_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{NON_BATCHED_WINDOW_LENGTH}s_{NON_BATCHED_SLIDING_LENGTH}s_{NON_BATCHED_SPLIT_LENGTH}s/NON_BATCHED_FEatures.hdf5'\n",
    "dataloader_params = {\n",
    "    'batch_size': 1,\n",
    "    'shuffle': False,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "val_losocv_loader = LOSOCVSensorDataLoader(NON_BATCHED_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "NON_BATCHED_DATASETS_PATH = val_losocv_loader.prepare_datasets(f'src/wesad/WESAD/datasets/wrist/all/{NON_BATCHED_WINDOW_LENGTH}s_{NON_BATCHED_SLIDING_LENGTH}s_{NON_BATCHED_SPLIT_LENGTH}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate Models with LOSOCV\n",
    "\n",
    "Now we can use the prepared datasets and form dataloaders which will then be used to perform LOSOCV on the models. Using the config file we can set the models that we want to test and their corresponding hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Subject: 2\n",
      "Train: 4867\n",
      "Val: 348\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 811/811 [01:02<00:00, 12.93it/s, loss=0.011]   \n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'val_metrics' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m\n\u001b[1;32m     72\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_path \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39msave_path\u001b[38;5;241m.\u001b[39mformat(fold\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# if i == 0:\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#     trainer.print_model_summary()\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Train the model on the batched data without the sliding co-attention buffer\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m trained_model_ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel checkpoint saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrained_model_ckpt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Now validate using the sliding co-attention buffer.  \u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/Imperial FYP/ Modular-Multimodal-Stress-Detector/src/ml_pipeline/train/pytorch_trainer.py:91\u001b[0m, in \u001b[0;36mPyTorchTrainer.train\u001b[0;34m(self, use_wandb, name_wandb)\u001b[0m\n\u001b[1;32m     89\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)\n\u001b[1;32m     90\u001b[0m avg_acc \u001b[38;5;241m=\u001b[39m epoch_correct \u001b[38;5;241m/\u001b[39m epoch_total\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, | training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, | training accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mval_metrics\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mNAME][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mNAME][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Log end of epoch training loss and accuracy to wandb\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_wandb:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'val_metrics' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from src.ml_pipeline.train import PyTorchTrainer\n",
    "from src.ml_pipeline.models.attention_models import MARCONet\n",
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "from src.ml_pipeline.utils import get_active_key, get_key, load_json, copy_json, get_values\n",
    "from src.utils import save_var\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "# CONFIG file for the model\n",
    "MARCO_CONFIG = 'config_files/model_training/deep/marco_config.json'\n",
    "\n",
    "# CONFIG file for the dataset\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "\n",
    "# Load Train Dataloaders for LOSOCV\n",
    "BATCHED_WINDOW_LENGTH = 30\n",
    "BATCHED_SPLIT_LENGTH = int(BATCHED_WINDOW_LENGTH / 6) # this will sub-split the data 6 times each of 5 seconds\n",
    "BATCHED_SLIDING_LENGTH = BATCHED_SPLIT_LENGTH # this will create 6 samples per 30 seconds since 30/5 = 6 with 5:1 ratio of synthetic to real samples\n",
    "BATCHED_WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{BATCHED_WINDOW_LENGTH}s_{BATCHED_SLIDING_LENGTH}s_{BATCHED_SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': get_values(MARCO_CONFIG, 'seq_length'),\n",
    "    'shuffle': True,\n",
    "    'drop_last': True\n",
    "}\n",
    "BATCHED_DATASETS_PATH = f'src/wesad/WESAD/datasets/wrist/all/{BATCHED_WINDOW_LENGTH}s_{BATCHED_SLIDING_LENGTH}s_{BATCHED_SPLIT_LENGTH}s/losocv_datasets.pkl'\n",
    "train_losocv_loader = LOSOCVSensorDataLoader(BATCHED_WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "train_dataloaders, train_input_dims = train_losocv_loader.get_data_loaders(BATCHED_DATASETS_PATH)\n",
    "\n",
    "# Load Val Dataloaders for LOSOCV\n",
    "NON_BATCHED_WINDOW_LENGTH = 5\n",
    "NON_BATCHED_SLIDING_LENGTH = NON_BATCHED_WINDOW_LENGTH # this will create no overlap between segments i.e. no augmented / synthetic data.\n",
    "NON_BATCHED_SPLIT_LENGTH = NON_BATCHED_WINDOW_LENGTH # this will not sub-split the data\n",
    "NON_BATCHED_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{NON_BATCHED_WINDOW_LENGTH}s_{NON_BATCHED_SLIDING_LENGTH}s_{NON_BATCHED_SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "dataloader_params = {\n",
    "    'batch_size': get_values(MARCO_CONFIG, 'batch_size'),\n",
    "    'shuffle': False,\n",
    "    'drop_last': True\n",
    "}\n",
    "NON_BATCHED_DATASETS_PATH = f'src/wesad/WESAD/datasets/wrist/all/{NON_BATCHED_WINDOW_LENGTH}s_{NON_BATCHED_SLIDING_LENGTH}s_{NON_BATCHED_SPLIT_LENGTH}s/losocv_datasets.pkl'\n",
    "val_losocv_loader = LOSOCVSensorDataLoader(NON_BATCHED_FE, WRIST_CONFIG, **dataloader_params)\n",
    "val_dataloaders, val_input_dims = val_losocv_loader.get_data_loaders(NON_BATCHED_DATASETS_PATH, val_only=True)\n",
    "\n",
    "assert train_input_dims == val_input_dims, 'Input dimensions of train and val dataloaders do not match'\n",
    "\n",
    "# Load Model Parameters\n",
    "model_config = load_json(MARCO_CONFIG)\n",
    "model_config = {\n",
    "    **model_config,\n",
    "    'input_dims': train_input_dims\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "results = []\n",
    "for (subject_id, train_loader), (_, val_loader) in zip(train_dataloaders.items(), val_dataloaders.items()):\n",
    "    train_loader_batched = train_loader['train']\n",
    "    val_loader_batched = train_loader['val']\n",
    "    val_loader = val_loader['val']\n",
    "    print(f'\\nSubject: {subject_id}')\n",
    "    print(f'Train: {len(train_loader_batched.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    model = MARCONet(**model_config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PyTorchTrainer(model, train_loader_batched, val_loader_batched, MARCO_CONFIG, device)\n",
    "    trainer.save_path = trainer.save_path.format(fold=f'subject_{subject_id}')\n",
    "    # if i == 0:\n",
    "    #     trainer.print_model_summary()\n",
    "    \n",
    "    # Train the model on the batched data without the sliding co-attention buffer\n",
    "    trained_model_ckpt = trainer.train()\n",
    "    print(f'Model checkpoint saved to: {trained_model_ckpt}\\n')\n",
    "\n",
    "    # Now validate using the sliding co-attention buffer.  \n",
    "    trainer.model.seq_length = get_values(MARCO_CONFIG, 'seq_length')\n",
    "    result = trainer.validate(trained_model_ckpt, subject_id, val_loader=val_loader)\n",
    "    results.append(result)\n",
    "    break\n",
    "\n",
    "# save the results to pkl\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path = f'src/wesad/WESAD/results/marco/wrist_results/{NON_BATCHED_WINDOW_LENGTH}s_{NON_BATCHED_SLIDING_LENGTH}s_{NON_BATCHED_SPLIT_LENGTH}s/{current_time}/generalized'\n",
    "save_var(results, f'{save_path}/results.pkl', 'Results')\n",
    "copy_json(MARCO_CONFIG, f'{save_path}/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "import os\n",
    "\n",
    "# results_path = f'{save_path}/results.pkl'\n",
    "# results_path = 'src/wesad/WESAD/results/san/wrist_results/10s_2s_10s/2024_06_29_02_48_46/generalized/results.pkl'\n",
    "results_path = 'src/wesad/WESAD/results/marco/wrist_results/5s_5s_5s/2024_07_01_11_48_16/generalized/results.pkl'\n",
    "results = load_var(results_path)\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective(os.path.dirname(results_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Dev_C11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
