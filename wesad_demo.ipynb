{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Modular Multimodal Data Fusion ML Pipeline for stress detection for the WESAD Database\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started:\n",
    "First, download necessary packages, if you are using a venv such as Conda, activate this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Installation\n",
    "If you are on Linux, run this cell to download and extract the WESAD dataset automatically, otherwise download manually [here](https://uni-siegen.sciebo.de/s/HGdUkoNlW1Ub0Gx/download) and unzip the `WESAD` file into the `wesad` directory i.e. `wesad/WESAD/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd src/wesad && bash download_database.sh\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This will automatically extract the biosensor data from the WESAD directory into several merged files in `.pkl` format.\n",
    "\n",
    "This will take around 10 minutes depending on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.wesad.data_preprocessing import WESADDataPreprocessor\n",
    "\n",
    "preprocessor = WESADDataPreprocessor('src/wesad/WESAD/')\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Preprocessing Steps\n",
    "We will preprocess each signal with their respective preprocessing steps:\n",
    "\n",
    "### Chest Signals\n",
    "\n",
    "#### ECG\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.7 Hz and 3.7 Hz.\n",
    "\n",
    "#### EMG\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth lowpass filter of order 3 with cutoff frequency 0.5 Hz.\n",
    "\n",
    "#### EDA\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth lowpass filter of order 2 with cutoff frequency 5 Hz.\n",
    "\n",
    "#### TEMP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "\n",
    "#### RESP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.1 Hz and 0.35 Hz.\n",
    "\n",
    "#### ACC\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 31 and order 5.\n",
    "\n",
    "### Wrist Signals\n",
    "\n",
    "#### BVP\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.7 Hz and 3.7 Hz.\n",
    "\n",
    "#### TEMP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "\n",
    "#### ACC\n",
    "- **Filtering**: Finite Impulse Response (FIR) filter with a length of 64 with a cut-off frequency of 0.4 Hz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config_files\n",
    "CHEST_CONFIG = 'src/wesad/wesad_chest_configuration.json'\n",
    "WRIST_CONFIG = 'src/wesad/wesad_wrist_configuration.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.preprocessing import SignalPreprocessor\n",
    "\n",
    "# preprocess the chest data\n",
    "signal_preprocessor = SignalPreprocessor('src/wesad/WESAD/raw/merged_chest.pkl', 'src/wesad/WESAD/cleaned/chest_preprocessed.pkl', CHEST_CONFIG)\n",
    "signal_preprocessor.preprocess_signals()\n",
    "\n",
    "# preprocess the wrist data\n",
    "signal_preprocessor = SignalPreprocessor('src/wesad/WESAD/raw/merged_wrist.pkl', 'src/wesad/WESAD/cleaned/wrist_preprocessed.pkl', WRIST_CONFIG, wrist=True)\n",
    "signal_preprocessor.preprocess_signals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning: Manual Feature Extraction\n",
    "\n",
    "During the feature extraction, data is loaded in an augmented manner using a 60-second window with a sliding length of 5 seconds.\n",
    "\n",
    "The manual feature extraction derives the following features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import DataAugmenter\n",
    "from src.ml_pipeline.feature_extraction import ManualFE\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "\n",
    "wrist_augmenter = DataAugmenter('src/wesad/WESAD/cleaned/wrist_preprocessed.pkl', WRIST_CONFIG)\n",
    "batches = wrist_augmenter.segment_data(WINDOW_LENGTH, SLIDING_LENGTH)\n",
    "\n",
    "manual_fe = ManualFE(batches, 'src/wesad/WESAD/manual_fe/wrist_manual_fe.hdf5', WRIST_CONFIG)\n",
    "manual_fe.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import DataAugmenter\n",
    "from src.ml_pipeline.feature_extraction import ManualFE\n",
    "\n",
    "CHEST_CONFIG = 'src/wesad/wesad_chest_configuration.json'\n",
    "WRIST_CONFIG = 'src/wesad/wesad_wrist_configuration.json'\n",
    "\n",
    "chest_augmenter = DataAugmenter('src/wesad/WESAD/cleaned/chest_preprocessed.pkl', CHEST_CONFIG)\n",
    "batches = chest_augmenter.segment_data()\n",
    "\n",
    "manual_fe = ManualFE(batches, 'src/wesad/WESAD/manual_fe/chest_manual_fe.hdf5', CHEST_CONFIG)\n",
    "manual_fe.extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning: Automatic Feature Extraction\n",
    "\n",
    "The automatic feature extraction uses autoencoders to derive features from its latent space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the preprocessed `.pkl` files we will make it into a dataloader - where LOSOCV (Leave one subject out cross validation). The data augmented samples will be used in the training set but ignored in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train: 49640\n",
      "Val: 288\n",
      "\n",
      "Fold 1\n",
      "Train: 49592\n",
      "Val: 288\n",
      "\n",
      "Fold 2\n",
      "Train: 49584\n",
      "Val: 288\n",
      "\n",
      "Fold 3\n",
      "Train: 49488\n",
      "Val: 288\n",
      "\n",
      "Fold 4\n",
      "Train: 49520\n",
      "Val: 296\n",
      "\n",
      "Fold 5\n",
      "Train: 49528\n",
      "Val: 304\n",
      "\n",
      "Fold 6\n",
      "Train: 49504\n",
      "Val: 304\n",
      "\n",
      "Fold 7\n",
      "Train: 49520\n",
      "Val: 280\n",
      "\n",
      "Fold 8\n",
      "Train: 49400\n",
      "Val: 304\n",
      "\n",
      "Fold 9\n",
      "Train: 49472\n",
      "Val: 304\n",
      "\n",
      "Fold 10\n",
      "Train: 49480\n",
      "Val: 304\n",
      "\n",
      "Fold 11\n",
      "Train: 49480\n",
      "Val: 304\n",
      "\n",
      "Fold 12\n",
      "Train: 49464\n",
      "Val: 312\n",
      "\n",
      "Fold 13\n",
      "Train: 49488\n",
      "Val: 296\n",
      "\n",
      "Fold 14\n",
      "Train: 49400\n",
      "Val: 304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVDataLoader\n",
    "\n",
    "# Set Parameters\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "CHEST_CONFIG = 'src/wesad/wesad_chest_configuration.json'\n",
    "WRIST_CONFIG = 'src/wesad/wesad_wrist_configuration.json'\n",
    "\n",
    "losocv_loader = LOSOCVDataLoader('src/wesad/WESAD/manual_fe/wrist_manual_fe.hdf5', WRIST_CONFIG, **params)\n",
    "dataloaders = losocv_loader.get_dataloaders()\n",
    "\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'Fold {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m WRIST_CONFIG \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc/wesad/wesad_wrist_configuration.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m losocv_loader \u001b[38;5;241m=\u001b[39m LOSOCVDataLoader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc/wesad/WESAD/manual_fe/chest_manual_fe.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m, CHEST_CONFIG, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 14\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m \u001b[43mlosocv_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (subject_id, loaders) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloaders\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m     17\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Dev/Imperial FYP/ Modular-Multimodal-Stress-Detector/src/ml_pipeline/data_loader/data_loader.py:81\u001b[0m, in \u001b[0;36mLOSOCVDataLoader.get_dataloaders\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataset(exclude_subject\u001b[38;5;241m=\u001b[39msubject_id, include_augmented\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataset(exclude_subject\u001b[38;5;241m=\u001b[39msubject_id, include_augmented\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 81\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m     84\u001b[0m dataloaders[subject_id] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loader, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: val_loader}\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev_C11/lib/python3.12/site-packages/torch/utils/data/dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev_C11/lib/python3.12/site-packages/torch/utils/data/sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVDataLoader\n",
    "\n",
    "# Set Parameters\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "CHEST_CONFIG = 'src/wesad/wesad_chest_configuration.json'\n",
    "WRIST_CONFIG = 'src/wesad/wesad_wrist_configuration.json'\n",
    "\n",
    "losocv_loader = LOSOCVDataLoader('src/wesad/WESAD/manual_fe/chest_manual_fe.hdf5', CHEST_CONFIG, **params)\n",
    "dataloaders = losocv_loader.get_dataloaders()\n",
    "\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'Fold {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming the LOSOCVDataLoader and other necessary classes are defined as provided\n",
    "\n",
    "# Example configuration and parameters\n",
    "CHEST_CONFIG = 'path/to/config'  # Replace with the actual path to the config file\n",
    "params = {'batch_size': 32, 'shuffle': True}\n",
    "\n",
    "# Initialize LOSOCVDataLoader\n",
    "losocv_loader = LOSOCVDataLoader('src/wesad/WESAD/manual_fe/chest_manual_fe.hdf5', CHEST_CONFIG, **params)\n",
    "dataloaders = losocv_loader.get_dataloaders()\n",
    "\n",
    "# Iterate through the dataloaders and train the model\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'Fold {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize and train the Random Forest model using H2O\n",
    "    rf_model = H2ORandomForestEstimator(ntrees=50, max_depth=20)\n",
    "    trainer = H2OTrainer(rf_model, train_loader, val_loader, target_column='target')\n",
    "    trainer.train()\n",
    "\n",
    "    # Example of predicting on validation data\n",
    "    predictions = trainer.predict(val_loader)\n",
    "    print(predictions)\n",
    "\n",
    "    # Shutdown H2O cluster\n",
    "    trainer.shutdown()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Dev_C11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
