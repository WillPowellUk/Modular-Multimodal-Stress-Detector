{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Modular Multimodal Data Fusion ML Pipeline for stress detection for the WESAD Database\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started:\n",
    "First, download necessary packages, if you are using a venv such as Conda, activate this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Installation\n",
    "If you are on Linux, run this cell to download and extract the WESAD dataset automatically, otherwise download manually [here](https://uni-siegen.sciebo.de/s/HGdUkoNlW1Ub0Gx/download) and unzip the `WESAD` file into the `wesad` directory i.e. `wesad/WESAD/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd src/wesad && bash download_database.sh\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This will automatically extract the biosensor data from the WESAD directory into several merged files in `.pkl` format.\n",
    "\n",
    "This will take around 10 minutes depending on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.wesad.data_preprocessing import WESADDataPreprocessor\n",
    "\n",
    "preprocessor = WESADDataPreprocessor('src/wesad/WESAD/')\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Preprocessing Steps\n",
    "We will preprocess each signal with their respective preprocessing steps:\n",
    "\n",
    "### Chest Signals\n",
    "\n",
    "#### ECG\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.7 Hz and 3.7 Hz.\n",
    "\n",
    "#### EMG\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth lowpass filter of order 3 with cutoff frequency 0.5 Hz.\n",
    "\n",
    "#### EDA\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth lowpass filter of order 2 with cutoff frequency 5 Hz.\n",
    "\n",
    "#### TEMP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "\n",
    "#### RESP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.1 Hz and 0.35 Hz.\n",
    "\n",
    "#### ACC\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 31 and order 5.\n",
    "\n",
    "### Wrist Signals\n",
    "\n",
    "#### BVP\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.7 Hz and 3.7 Hz.\n",
    "\n",
    "#### TEMP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "\n",
    "#### ACC\n",
    "- **Filtering**: Finite Impulse Response (FIR) filter with a length of 64 with a cut-off frequency of 0.4 Hz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config_files\n",
    "CHEST_CONFIG = 'src/wesad/wesad_chest_configuration.json'\n",
    "WRIST_CONFIG = 'src/wesad/wesad_wrist_configuration.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.preprocessing import SignalPreprocessor\n",
    "\n",
    "# preprocess the chest data\n",
    "signal_preprocessor = SignalPreprocessor('src/wesad/WESAD/raw/merged_chest.pkl', 'src/wesad/WESAD/cleaned/chest_preprocessed.pkl', CHEST_CONFIG)\n",
    "signal_preprocessor.preprocess_signals()\n",
    "\n",
    "# preprocess the wrist data\n",
    "signal_preprocessor = SignalPreprocessor('src/wesad/WESAD/raw/merged_wrist.pkl', 'src/wesad/WESAD/cleaned/wrist_preprocessed.pkl', WRIST_CONFIG, wrist=True)\n",
    "signal_preprocessor.preprocess_signals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning: Manual Feature Extraction\n",
    "\n",
    "During the feature extraction, data is loaded in an augmented manner using a 60-second window with a sliding length of 5 seconds.\n",
    "\n",
    "The manual feature extraction derives the following features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import DataAugmenter\n",
    "from src.ml_pipeline.feature_extraction import ManualFE\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "\n",
    "wrist_augmenter = DataAugmenter('src/wesad/WESAD/cleaned/wrist_preprocessed.pkl', WRIST_CONFIG)\n",
    "batches = wrist_augmenter.segment_data(WINDOW_LENGTH, SLIDING_LENGTH)\n",
    "\n",
    "manual_fe = ManualFE(batches, 'src/wesad/WESAD/manual_fe/wrist_manual_fe.hdf5', WRIST_CONFIG)\n",
    "manual_fe.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import DataAugmenter\n",
    "from src.ml_pipeline.feature_extraction import ManualFE\n",
    "\n",
    "CHEST_CONFIG = 'src/wesad/wesad_chest_configuration.json'\n",
    "WRIST_CONFIG = 'src/wesad/wesad_wrist_configuration.json'\n",
    "\n",
    "chest_augmenter = DataAugmenter('src/wesad/WESAD/cleaned/chest_preprocessed.pkl', CHEST_CONFIG)\n",
    "batches = chest_augmenter.segment_data()\n",
    "\n",
    "manual_fe = ManualFE(batches, 'src/wesad/WESAD/manual_fe/chest_manual_fe.hdf5', CHEST_CONFIG)\n",
    "manual_fe.extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning: Automatic Feature Extraction\n",
    "\n",
    "The automatic feature extraction uses autoencoders to derive features from its latent space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the preprocessed `.pkl` files we will make it into a dataloader - where LOSOCV (Leave one subject out cross validation). The data augmented samples will be used in the training set but ignored in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train: 49640\n",
      "Val: 288\n",
      "\n",
      "Fold 1\n",
      "Train: 49592\n",
      "Val: 288\n",
      "\n",
      "Fold 2\n",
      "Train: 49584\n",
      "Val: 288\n",
      "\n",
      "Fold 3\n",
      "Train: 49488\n",
      "Val: 288\n",
      "\n",
      "Fold 4\n",
      "Train: 49520\n",
      "Val: 296\n",
      "\n",
      "Fold 5\n",
      "Train: 49528\n",
      "Val: 304\n",
      "\n",
      "Fold 6\n",
      "Train: 49504\n",
      "Val: 304\n",
      "\n",
      "Fold 7\n",
      "Train: 49520\n",
      "Val: 280\n",
      "\n",
      "Fold 8\n",
      "Train: 49400\n",
      "Val: 304\n",
      "\n",
      "Fold 9\n",
      "Train: 49472\n",
      "Val: 304\n",
      "\n",
      "Fold 10\n",
      "Train: 49480\n",
      "Val: 304\n",
      "\n",
      "Fold 11\n",
      "Train: 49480\n",
      "Val: 304\n",
      "\n",
      "Fold 12\n",
      "Train: 49464\n",
      "Val: 312\n",
      "\n",
      "Fold 13\n",
      "Train: 49488\n",
      "Val: 296\n",
      "\n",
      "Fold 14\n",
      "Train: 49400\n",
      "Val: 304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVDataLoader\n",
    "\n",
    "# Set Parameters\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "CHEST_CONFIG = 'src/wesad/wesad_chest_configuration.json'\n",
    "WRIST_CONFIG = 'src/wesad/wesad_wrist_configuration.json'\n",
    "\n",
    "losocv_loader = LOSOCVDataLoader('src/wesad/WESAD/manual_fe/wrist_manual_fe.hdf5', WRIST_CONFIG, **params)\n",
    "dataloaders = losocv_loader.get_dataloaders()\n",
    "\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'Fold {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train: 72552\n",
      "Val: 420\n",
      "\n",
      "Fold 1\n",
      "Train: 72480\n",
      "Val: 420\n",
      "\n",
      "Fold 2\n",
      "Train: 72444\n",
      "Val: 432\n",
      "\n",
      "Fold 3\n",
      "Train: 72324\n",
      "Val: 432\n",
      "\n",
      "Fold 4\n",
      "Train: 72360\n",
      "Val: 432\n",
      "\n",
      "Fold 5\n",
      "Train: 72372\n",
      "Val: 432\n",
      "\n",
      "Fold 6\n",
      "Train: 72348\n",
      "Val: 432\n",
      "\n",
      "Fold 7\n",
      "Train: 72372\n",
      "Val: 432\n",
      "\n",
      "Fold 8\n",
      "Train: 72180\n",
      "Val: 444\n",
      "\n",
      "Fold 9\n",
      "Train: 72300\n",
      "Val: 444\n",
      "\n",
      "Fold 10\n",
      "Train: 72300\n",
      "Val: 444\n",
      "\n",
      "Fold 11\n",
      "Train: 72300\n",
      "Val: 444\n",
      "\n",
      "Fold 12\n",
      "Train: 72288\n",
      "Val: 444\n",
      "\n",
      "Fold 13\n",
      "Train: 72312\n",
      "Val: 444\n",
      "\n",
      "Fold 14\n",
      "Train: 72180\n",
      "Val: 444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVDataLoader\n",
    "\n",
    "# Set Parameters\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "CHEST_CONFIG = 'src/wesad/wesad_chest_configuration.json'\n",
    "WRIST_CONFIG = 'src/wesad/wesad_wrist_configuration.json'\n",
    "\n",
    "losocv_loader = LOSOCVDataLoader('src/wesad/WESAD/manual_fe/chest_manual_fe.hdf5', CHEST_CONFIG, **params)\n",
    "dataloaders = losocv_loader.get_dataloaders()\n",
    "\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'Fold {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming the LOSOCVDataLoader and other necessary classes are defined as provided\n",
    "\n",
    "# Example configuration and parameters\n",
    "CHEST_CONFIG = 'path/to/config'  # Replace with the actual path to the config file\n",
    "params = {'batch_size': 32, 'shuffle': True}\n",
    "\n",
    "# Initialize LOSOCVDataLoader\n",
    "losocv_loader = LOSOCVDataLoader('src/wesad/WESAD/manual_fe/chest_manual_fe.hdf5', CHEST_CONFIG, **params)\n",
    "dataloaders = losocv_loader.get_dataloaders()\n",
    "\n",
    "# Iterate through the dataloaders and train the model\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'Fold {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize and train the Random Forest model using H2O\n",
    "    rf_model = H2ORandomForestEstimator(ntrees=50, max_depth=20)\n",
    "    trainer = H2OTrainer(rf_model, train_loader, val_loader, target_column='target')\n",
    "    trainer.train()\n",
    "\n",
    "    # Example of predicting on validation data\n",
    "    predictions = trainer.predict(val_loader)\n",
    "    print(predictions)\n",
    "\n",
    "    # Shutdown H2O cluster\n",
    "    trainer.shutdown()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Dev_C11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
