{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Modular Multimodal Data Fusion ML Pipeline for stress detection for the WESAD Database\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started:\n",
    "First, download necessary packages, if you are using a venv such as Conda, activate this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'child' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/IPython/utils/_process_posix.py:151\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[43mpexpect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    152\u001b[0m flush \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_poll \u001b[38;5;241m=\u001b[39m use_poll\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc\u001b[38;5;241m.\u001b[39mpid\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPtyProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/ptyprocess/ptyprocess.py:315\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    314\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_write)\n\u001b[0;32m--> 315\u001b[0m exec_err_data \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_read)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstall -r requirements.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/IPython/core/magics/packaging.py:92\u001b[0m, in \u001b[0;36mPackagingMagics.pip\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     python \u001b[38;5;241m=\u001b[39m shlex\u001b[38;5;241m.\u001b[39mquote(python)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpython\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote: you may need to restart the kernel to use updated packages.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/IPython/utils/_process_posix.py:167\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    162\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241m.\u001b[39msendline(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'child' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Installation\n",
    "If you are on Linux, run this cell to download and extract the WESAD dataset automatically, otherwise download manually [here](https://uni-siegen.sciebo.de/s/HGdUkoNlW1Ub0Gx/download) and unzip the `WESAD` file into the `wesad` directory i.e. `wesad/WESAD/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd src/wesad && bash download_database.sh\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This will automatically extract the biosensor data from the WESAD directory into several merged files in `.pkl` format.\n",
    "\n",
    "This will take around 10 minutes depending on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.wesad.data_preprocessing import WESADDataPreprocessor\n",
    "\n",
    "preprocessor = WESADDataPreprocessor('src/wesad/WESAD/')\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Preprocessing Steps\n",
    "We will preprocess each signal with their respective preprocessing steps:\n",
    "\n",
    "### Chest Signals\n",
    "\n",
    "#### ECG\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.7 Hz and 3.7 Hz.\n",
    "\n",
    "#### EMG\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth lowpass filter of order 3 with cutoff frequency 0.5 Hz.\n",
    "\n",
    "#### EDA\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth lowpass filter of order 2 with cutoff frequency 5 Hz.\n",
    "\n",
    "#### TEMP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "\n",
    "#### RESP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.1 Hz and 0.35 Hz.\n",
    "\n",
    "#### ACC\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 31 and order 5.\n",
    "\n",
    "### Wrist Signals\n",
    "\n",
    "#### BVP\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.7 Hz and 3.7 Hz.\n",
    "\n",
    "#### TEMP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "\n",
    "#### ACC\n",
    "- **Filtering**: Finite Impulse Response (FIR) filter with a length of 64 with a cut-off frequency of 0.4 Hz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config_files\n",
    "CHEST_CONFIG = 'config_files/dataset/wesad_chest_configuration.json'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.preprocessing import SignalPreprocessor\n",
    "\n",
    "# preprocess the chest data\n",
    "signal_preprocessor = SignalPreprocessor('src/wesad/WESAD/raw/merged_chest.pkl', 'src/wesad/WESAD/cleaned/chest_preprocessed.pkl', CHEST_CONFIG)\n",
    "signal_preprocessor.preprocess_signals()\n",
    "\n",
    "# preprocess the wrist data\n",
    "signal_preprocessor = SignalPreprocessor('src/wesad/WESAD/raw/merged_wrist.pkl', 'src/wesad/WESAD/cleaned/wrist_preprocessed.pkl', WRIST_CONFIG, wrist=True)\n",
    "signal_preprocessor.preprocess_signals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation and Splitting\n",
    "Data augmentation will take the form of a sliding window. Once the data is augmented, each sample will then be split into smaller segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import DataAugmenter\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "\n",
    "wrist_augmenter = DataAugmenter('src/wesad/WESAD/cleaned/wrist_preprocessed.pkl', WRIST_CONFIG) \n",
    "batches = wrist_augmenter.augment_data(WINDOW_LENGTH, SLIDING_LENGTH)\n",
    "wrist_splitted_segments = wrist_augmenter.split_segments(batches, WINDOW_LENGTH//SPLIT_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning\n",
    "\n",
    "The manual feature extraction derives features in the time, frequency and non-linear domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.feature_extraction import ManualFE\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "\n",
    "manual_fe = ManualFE(wrist_splitted_segments, WRIST_FE, WRIST_CONFIG)\n",
    "manual_fe.extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LOSOCV Datasets\n",
    "\n",
    "Now, using the preprocessed `.pkl` files we will make it into a dataloader with LOSOCV (Leave one subject out cross validation). The data augmented samples will be used in the training set but ignored in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVDataLoader\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "# WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/test203/wrist_manual_fe_{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s.hdf5'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "losocv_loader = LOSOCVDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "DATASETS_PATH = losocv_loader.prepare_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate Models with LOSOCV\n",
    "\n",
    "Now we can use the prepared datasets and form dataloaders which will then be used to perform LOSOCV on the models. Using the config file we can set the models that we want to test and their corresponding hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVDataLoader\n",
    "from src.ml_pipeline.train import TraditionalMLTrainer\n",
    "from src.utils import save_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load tradtional model config\n",
    "losocv_loader = LOSOCVDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders = losocv_loader.get_data_loaders(DATASETS_PATH)\n",
    "TRADTIONAL_ML_CONFIG = 'config_files/model_training/traditional/traditional_models.json'\n",
    "\n",
    "results = []\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'\\nFold {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = TraditionalMLTrainer(TRADTIONAL_ML_CONFIG, train_loader, val_loader)\n",
    "\n",
    "    # trained_models = trainer.tune_hyperparameters(n_jobs=4, cv=None, verbose=2)\n",
    "    trained_models = trainer.train()\n",
    "\n",
    "    result = trainer.validate(trained_models)\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "# save the results to pkl\n",
    "save_var(results, 'src/wesad/WESAD/results/traditional_models/wrist_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "results = load_var('src/wesad/WESAD/results/traditional_models/wrist_results.pkl')\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Network Alongside Manual Feature Extraction\n",
    "This network employs self-attention networks for intra-modality feature extraction before applying late fusion between modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LOSOCV Datasets on a Per Sensor Basis\n",
    "\n",
    "Now, using the preprocessed `.pkl` files we will make it into a dataloader with LOSOCV (Leave one subject out cross validation) on a per sensor basis. The data augmented samples will be used in the training set but ignored in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "losocv_loader = LOSOCVSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "DATASETS_PATH = losocv_loader.prepare_datasets(f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate Models with LOSOCV\n",
    "\n",
    "Now we can use the prepared datasets and form dataloaders which will then be used to perform LOSOCV on the models. Using the config file we can set the models that we want to test and their corresponding hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.train import PyTorchTrainer\n",
    "from src.ml_pipeline.models.attention_models import ModularModalityFusionNet\n",
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "from src.ml_pipeline.utils import get_active_key, get_key, load_json, copy_json\n",
    "from src.utils import save_var\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}_mini/wrist_manual_fe_{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}.hdf5'\n",
    "DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/losocv_datasets.pkl'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s_mini/wrist_features.hdf5'\n",
    "SAN_MODEL_CONFIG = 'config_files/model_training/deep/modular_modality_fusion_net_config.json'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load Dataloaders for LOSOCV\n",
    "losocv_loader = LOSOCVSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders, input_dims = losocv_loader.get_data_loaders(DATASETS_PATH)\n",
    "\n",
    "# Load Model Parameters\n",
    "model_config = load_json(SAN_MODEL_CONFIG)\n",
    "model_config = {\n",
    "    **model_config,\n",
    "    'input_dims': input_dims\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "results = []\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'\\nSubject: {subject_id}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    model = ModularModalityFusionNet(**model_config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PyTorchTrainer(model, train_loader, val_loader, SAN_MODEL_CONFIG, device)\n",
    "    trainer.save_path = trainer.save_path.format(fold=f'subject_{subject_id}')\n",
    "    trained_model_ckpt = trainer.train()\n",
    "    print(f'Model checkpoint saved to: {trained_model_ckpt}\\n')\n",
    "\n",
    "    result = trainer.validate(trained_model_ckpt)\n",
    "    results.append(result)\n",
    "    break\n",
    "\n",
    "# save the results to pkl\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path = f'src/wesad/WESAD/results/san/wrist_results/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/{current_time}/generalized'\n",
    "save_var(results, f'{save_path}/results.pkl', 'Results')\n",
    "copy_json(SAN_MODEL_CONFIG, f'{save_path}/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# results_path = 'src/wesad/WESAD/results/san/wrist_results/60s_5s_10s/2024_06_20_14_55_01/.pkl'\n",
    "results_path = f'{save_path}/results.pkl'\n",
    "results = load_var(results_path)\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Cross Validation Dataloader for Personalization\n",
    "\n",
    "First we must prepare a new dataloader which exclusively contains one subject's personalized data that is used to cross validate the personalized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import PersonalSensorDataLoader\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "SUBJECT_ID = 2\n",
    "personal_loader = PersonalSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "DATASETS_PATH = personal_loader.prepare_datasets(f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s', SUBJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Personalization via Transfer Learning\n",
    "\n",
    "This will then be used to fine tune the model with the new multiheaded attention blocks architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.train import PyTorchTrainer\n",
    "from src.ml_pipeline.models.attention_models import PersonalizedModalityFusionNet, ModularModalityFusionNet\n",
    "from src.ml_pipeline.data_loader import PersonalSensorDataLoader\n",
    "from src.ml_pipeline.utils import get_active_key, load_json, copy_json\n",
    "from src.utils import save_var\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "SUBJECT_ID = 2\n",
    "\n",
    "DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/subject_{SUBJECT_ID}/personal_dataset.pkl'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "SAN_MODEL_PER_CONFIG = 'config_files/model_training/deep/personalized_modality_fusion_net_config.json'\n",
    "MFN_CKPT_PATH = 'src/wesad/WESAD/ckpts/san/wrist_manual_fe/60s_5s_10s/generalized/subject_2/checkpoint_5.pth'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load Dataloaders for LOSOCV\n",
    "personal_loader = PersonalSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders, input_dims = personal_loader.get_data_loaders(DATASETS_PATH)\n",
    "\n",
    "# Load Model Parameters\n",
    "model_config = load_json(SAN_MODEL_PER_CONFIG)\n",
    "model_config = {\n",
    "    **model_config,\n",
    "    'input_dims': input_dims\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "results = []\n",
    "for i, loaders in enumerate(dataloaders):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']    \n",
    "    print(f'\\nFold: {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    model = PersonalizedModalityFusionNet(MFN_CKPT_PATH, ModularModalityFusionNet, **model_config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PyTorchTrainer(model, train_loader, val_loader, SAN_MODEL_PER_CONFIG, device)\n",
    "    trainer.save_path = trainer.save_path.format(fold=f'fold_{i}')\n",
    "    trained_model_ckpt = trainer.train()\n",
    "    print(f'Model saved at: {trained_model_ckpt}')\n",
    "\n",
    "    result = trainer.validate(trained_model_ckpt)\n",
    "    results.append(result)\n",
    "\n",
    "# save the results to pkl\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path = f'src/wesad/WESAD/results/san/wrist_results/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/{current_time}/personalized'\n",
    "save_var(results, f'{save_path}/results.pkl', 'results')\n",
    "copy_json(SAN_MODEL_PER_CONFIG, f'{save_path}/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "results_path = f'{save_path}/results.pkl'\n",
    "results = load_var(results_path)\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self and Cross Attention Network\n",
    "\n",
    "This model now integrates cross attention into the self-attention network for inter-modality and intra-modality feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Subject: 2\n",
      "Train: 4821\n",
      "Val: 28\n",
      "\n",
      "Storing tensorboard log to: src/wesad/WESAD/ckpts/bcsa/wrist_manual_fe/60s_5s_10s/mmfn/generalized/subject_2/tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 151/151 [00:36<00:00,  4.18it/s, loss=0.0208] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, | training loss: 0.1486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 151/151 [00:34<00:00,  4.32it/s, loss=0.00951] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, | training loss: 0.0474\n",
      "Model checkpoint saved to: src/wesad/WESAD/ckpts/bcsa/wrist_manual_fe/60s_5s_10s/mmfn/generalized/subject_2/checkpoint_2.pth\n",
      "\n",
      "Variable 'Results' saved to src/wesad/WESAD/results/bcsa/wrist_results/60s_5s_10s/2024_06_21_15_50_27/generalized/results.pkl\n"
     ]
    }
   ],
   "source": [
    "from src.ml_pipeline.train import PyTorchTrainer\n",
    "from src.ml_pipeline.models.attention_models import ModularBCSA\n",
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "from src.ml_pipeline.utils import load_json, copy_json\n",
    "from src.utils import save_var\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}_mini/wrist_manual_fe_{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}.hdf5'\n",
    "DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/losocv_datasets.pkl'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s_mini/wrist_features.hdf5'\n",
    "BCSA_MODEL_CONFIG = 'config_files/model_training/deep/bsca_config.json'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load Dataloaders for LOSOCV\n",
    "losocv_loader = LOSOCVSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders, input_dims = losocv_loader.get_data_loaders(DATASETS_PATH)\n",
    "\n",
    "# Load Model Parameters\n",
    "model_config = load_json(BCSA_MODEL_CONFIG)\n",
    "model_config = {\n",
    "    **model_config,\n",
    "    'input_dims': input_dims\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "results = []\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'\\nSubject: {subject_id}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    model = ModularBCSA(**model_config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PyTorchTrainer(model, train_loader, val_loader, BCSA_MODEL_CONFIG, device)\n",
    "    trainer.save_path = trainer.save_path.format(fold=f'subject_{subject_id}')\n",
    "    trained_model_ckpt = trainer.train()\n",
    "    print(f'Model checkpoint saved to: {trained_model_ckpt}\\n')\n",
    "\n",
    "    result = trainer.validate(trained_model_ckpt)\n",
    "    results.append(result)\n",
    "    break\n",
    "\n",
    "# save the results to pkl\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path = f'src/wesad/WESAD/results/bcsa/wrist_results/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/{current_time}/generalized'\n",
    "save_var(results, f'{save_path}/results.pkl', 'Results')\n",
    "copy_json(BCSA_MODEL_CONFIG, f'{save_path}/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# results_path = f'{save_path}/results.pkl'\n",
    "results_path = 'src/wesad/WESAD/results/bcsa/wrist_results/60s_5s_10s/2024_06_21_14_38_49/generalized/results.pkl'\n",
    "results = load_var(results_path)\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Dev_C11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
