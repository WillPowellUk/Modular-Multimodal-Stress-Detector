{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Modular Multimodal Data Fusion ML Pipeline for stress detection for the WESAD Database\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started:\n",
    "First, download necessary packages, if you are using a venv such as Conda, activate this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Installation\n",
    "If you are on Linux, run this cell to download and extract the WESAD dataset automatically, otherwise download manually [here](https://uni-siegen.sciebo.de/s/HGdUkoNlW1Ub0Gx/download) and unzip the `WESAD` file into the `wesad` directory i.e. `wesad/WESAD/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd src/wesad && bash download_database.sh\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This will automatically extract the biosensor data from the WESAD directory into several merged files in `.pkl` format.\n",
    "\n",
    "This will take around 10 minutes depending on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.wesad.data_preprocessing import WESADDataPreprocessor\n",
    "\n",
    "preprocessor = WESADDataPreprocessor('src/wesad/WESAD/')\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Preprocessing Steps\n",
    "We will preprocess each signal with their respective preprocessing steps:\n",
    "\n",
    "### Chest Signals\n",
    "\n",
    "#### ECG\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.7 Hz and 3.7 Hz.\n",
    "\n",
    "#### EMG\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth lowpass filter of order 3 with cutoff frequency 0.5 Hz.\n",
    "\n",
    "#### EDA\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth lowpass filter of order 2 with cutoff frequency 5 Hz.\n",
    "\n",
    "#### TEMP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "\n",
    "#### RESP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.1 Hz and 0.35 Hz.\n",
    "\n",
    "#### ACC\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 31 and order 5.\n",
    "\n",
    "### Wrist Signals\n",
    "\n",
    "#### BVP\n",
    "- **Filtering**: Butterworth band-pass filter of order 3 with cutoff frequencies 0.7 Hz and 3.7 Hz.\n",
    "\n",
    "#### TEMP\n",
    "- **Smoothing**: Savitzky–Golay filter with window size 11 and order 3.\n",
    "\n",
    "#### ACC\n",
    "- **Filtering**: Finite Impulse Response (FIR) filter with a length of 64 with a cut-off frequency of 0.4 Hz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config_files\n",
    "CHEST_CONFIG = 'config_files/dataset/wesad_chest_configuration.json'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.preprocessing import SignalPreprocessor\n",
    "\n",
    "# preprocess the chest data\n",
    "signal_preprocessor = SignalPreprocessor('src/wesad/WESAD/raw/merged_chest.pkl', 'src/wesad/WESAD/cleaned/chest_preprocessed.pkl', CHEST_CONFIG)\n",
    "signal_preprocessor.preprocess_signals()\n",
    "\n",
    "# preprocess the wrist data\n",
    "signal_preprocessor = SignalPreprocessor('src/wesad/WESAD/raw/merged_wrist.pkl', 'src/wesad/WESAD/cleaned/wrist_preprocessed.pkl', WRIST_CONFIG, wrist=True)\n",
    "signal_preprocessor.preprocess_signals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation and Splitting\n",
    "Data augmentation will take the form of a sliding window. Once the data is augmented, each sample will then be split into smaller segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting data...\n",
      "Number of segments: 13635\n",
      "Synthetic to non-synthetic ratio: 4:1\n",
      "Splitting segments...\n",
      "Splitting complete.\n"
     ]
    }
   ],
   "source": [
    "from src.ml_pipeline.data_loader import DataAugmenter\n",
    "\n",
    "# WINDOW_LENGTH = 60\n",
    "# SLIDING_LENGTH = 5 # this will create 12 segments per minute since 60/5 = 12 with 11:1 ratio of synthetic to real samples\n",
    "# SPLIT_LENGTH = 10 # this will sub-split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "WINDOW_LENGTH = 10\n",
    "SLIDING_LENGTH = 2 # this will create 5 segments per 10 seconds since 10/2 = 5 with 4:1 ratio of synthetic to real samples\n",
    "SPLIT_LENGTH = WINDOW_LENGTH # this will not sub-split the data\n",
    "\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "\n",
    "wrist_augmenter = DataAugmenter('src/wesad/WESAD/cleaned/wrist_preprocessed.pkl', WRIST_CONFIG) \n",
    "batches = wrist_augmenter.augment_data(WINDOW_LENGTH, SLIDING_LENGTH)\n",
    "wrist_splitted_segments = wrist_augmenter.split_segments(batches, WINDOW_LENGTH//SPLIT_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning\n",
    "\n",
    "The manual feature extraction derives features in the time, frequency and non-linear domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features. Writing to log file: manual_fe.log...\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m WRIST_FE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc/wesad/WESAD/manual_fe/wrist_manual_fe/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWINDOW_LENGTH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSLIDING_LENGTH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSPLIT_LENGTH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms/wrist_features.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m manual_fe \u001b[38;5;241m=\u001b[39m ManualFE(wrist_splitted_segments, WRIST_FE, WRIST_CONFIG)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmanual_fe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/Imperial FYP/ Modular-Multimodal-Stress-Detector/src/ml_pipeline/feature_extraction/manual/manual_fe.py:223\u001b[0m, in \u001b[0;36mManualFE.extract_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m--> 223\u001b[0m     split_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features_from_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     batch_features\u001b[38;5;241m.\u001b[39mappend(split_features)\n\u001b[1;32m    225\u001b[0m all_batches_features\u001b[38;5;241m.\u001b[39mappend(batch_features)\n",
      "File \u001b[0;32m~/Dev/Imperial FYP/ Modular-Multimodal-Stress-Detector/src/ml_pipeline/feature_extraction/manual/manual_fe.py:48\u001b[0m, in \u001b[0;36mManualFE.extract_features_from_split\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m     46\u001b[0m     features_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_eda\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m eda_features\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbvp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m     bvp_features \u001b[38;5;241m=\u001b[39m \u001b[43mBVPFeatureExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbvp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     features_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbvp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m bvp_features\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_temp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Dev/Imperial FYP/ Modular-Multimodal-Stress-Detector/src/ml_pipeline/feature_extraction/manual/bvp_feature_extractor/bvp_feature_extractor.py:15\u001b[0m, in \u001b[0;36mBVPFeatureExtractor.extract_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m bvp_signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbvp_data\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Process BVP signal to find peaks\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m signals, info \u001b[38;5;241m=\u001b[39m \u001b[43mnk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mppg_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbvp_signal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Extract time domain features\u001b[39;00m\n\u001b[1;32m     18\u001b[0m time_domain \u001b[38;5;241m=\u001b[39m nk\u001b[38;5;241m.\u001b[39mhrv_time(signals, sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_rate)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/neurokit2/ppg/ppg_process.py:102\u001b[0m, in \u001b[0;36mppg_process\u001b[0;34m(ppg_signal, sampling_rate, method, method_quality, report, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m rate \u001b[38;5;241m=\u001b[39m signal_rate(\n\u001b[1;32m     98\u001b[0m     info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPG_Peaks\u001b[39m\u001b[38;5;124m\"\u001b[39m], sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate, desired_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(ppg_cleaned)\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Assess signal quality\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m quality \u001b[38;5;241m=\u001b[39m \u001b[43mppg_quality\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppg_cleaned\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppg_pw_peaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPG_Peaks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethods\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod_quality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethods\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs_quality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Prepare output\u001b[39;00m\n\u001b[1;32m    111\u001b[0m signals \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    112\u001b[0m     {\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPG_Raw\u001b[39m\u001b[38;5;124m\"\u001b[39m: ppg_signal,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     }\n\u001b[1;32m    119\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/neurokit2/ppg/ppg_quality.py:84\u001b[0m, in \u001b[0;36mppg_quality\u001b[0;34m(ppg_cleaned, ppg_pw_peaks, sampling_rate, method, approach)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Run selected quality assessment method\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplatematch\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 84\u001b[0m     quality \u001b[38;5;241m=\u001b[39m \u001b[43m_ppg_quality_templatematch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mppg_cleaned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppg_pw_peaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mppg_pw_peaks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     88\u001b[0m     quality \u001b[38;5;241m=\u001b[39m _ppg_quality_disimilarity(\n\u001b[1;32m     89\u001b[0m         ppg_cleaned, ppg_pw_peaks\u001b[38;5;241m=\u001b[39mppg_pw_peaks, sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate\n\u001b[1;32m     90\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/neurokit2/ppg/ppg_quality.py:136\u001b[0m, in \u001b[0;36m_ppg_quality_templatematch\u001b[0;34m(ppg_cleaned, ppg_pw_peaks, sampling_rate)\u001b[0m\n\u001b[1;32m    134\u001b[0m cc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(ppg_pw_peaks)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m beat_no \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(ppg_pw_peaks)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 136\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrcoef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpw_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbeat_no\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtempl_pw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     cc[beat_no] \u001b[38;5;241m=\u001b[39m temp[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Interpolate beat-by-beat CCs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/numpy/lib/function_base.py:2889\u001b[0m, in \u001b[0;36mcorrcoef\u001b[0;34m(x, y, rowvar, bias, ddof, dtype)\u001b[0m\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;129;01mor\u001b[39;00m ddof \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue:\n\u001b[1;32m   2886\u001b[0m     \u001b[38;5;66;03m# 2015-03-15, 1.10\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias and ddof have no effect and are deprecated\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2888\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m-> 2889\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mcov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2891\u001b[0m     d \u001b[38;5;241m=\u001b[39m diag(c)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/numpy/lib/function_base.py:2747\u001b[0m, in \u001b[0;36mcov\u001b[0;34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[0m\n\u001b[1;32m   2745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2746\u001b[0m     X_T \u001b[38;5;241m=\u001b[39m (X\u001b[38;5;241m*\u001b[39mw)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m-> 2747\u001b[0m c \u001b[38;5;241m=\u001b[39m dot(X, \u001b[43mX_T\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2748\u001b[0m c \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtrue_divide(\u001b[38;5;241m1\u001b[39m, fact)\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.ml_pipeline.feature_extraction import ManualFE\n",
    "\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "\n",
    "manual_fe = ManualFE(wrist_splitted_segments, WRIST_FE, WRIST_CONFIG)\n",
    "manual_fe.extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LOSOCV Datasets\n",
    "\n",
    "Now, using the preprocessed `.pkl` files we will make it into a dataloader with LOSOCV (Leave one subject out cross validation). The data augmented samples will be used in the training set but ignored in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVDataLoader\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "# WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/test203/wrist_manual_fe_{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s.hdf5'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "losocv_loader = LOSOCVDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "DATASETS_PATH = losocv_loader.prepare_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate Models with LOSOCV\n",
    "\n",
    "Now we can use the prepared datasets and form dataloaders which will then be used to perform LOSOCV on the models. Using the config file we can set the models that we want to test and their corresponding hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVDataLoader\n",
    "from src.ml_pipeline.train import TraditionalMLTrainer\n",
    "from src.utils import save_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load tradtional model config\n",
    "losocv_loader = LOSOCVDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders = losocv_loader.get_data_loaders(DATASETS_PATH)\n",
    "TRADTIONAL_ML_CONFIG = 'config_files/model_training/traditional/traditional_models.json'\n",
    "\n",
    "results = []\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'\\nFold {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = TraditionalMLTrainer(TRADTIONAL_ML_CONFIG, train_loader, val_loader)\n",
    "\n",
    "    # trained_models = trainer.tune_hyperparameters(n_jobs=4, cv=None, verbose=2)\n",
    "    trained_models = trainer.train()\n",
    "\n",
    "    result = trainer.validate(trained_models)\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "# save the results to pkl\n",
    "save_var(results, 'src/wesad/WESAD/results/traditional_models/wrist_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "results = load_var('src/wesad/WESAD/results/traditional_models/wrist_results.pkl')\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Network Alongside Manual Feature Extraction\n",
    "This network employs self-attention networks for intra-modality feature extraction before applying late fusion between modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LOSOCV Datasets on a Per Sensor Basis\n",
    "\n",
    "Now, using the preprocessed `.pkl` files we will make it into a dataloader with LOSOCV (Leave one subject out cross validation) on a per sensor basis. The data augmented samples will be used in the training set but ignored in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "losocv_loader = LOSOCVSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "DATASETS_PATH = losocv_loader.prepare_datasets(f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate Models with LOSOCV\n",
    "\n",
    "Now we can use the prepared datasets and form dataloaders which will then be used to perform LOSOCV on the models. Using the config file we can set the models that we want to test and their corresponding hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.train import PyTorchTrainer\n",
    "from src.ml_pipeline.models.attention_models import ModularModalityFusionNet\n",
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "from src.ml_pipeline.utils import get_active_key, get_key, load_json, copy_json\n",
    "from src.utils import save_var\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}_mini/wrist_manual_fe_{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}.hdf5'\n",
    "DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/losocv_datasets.pkl'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s_mini/wrist_features.hdf5'\n",
    "SAN_MODEL_CONFIG = 'config_files/model_training/deep/modular_modality_fusion_net_config.json'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load Dataloaders for LOSOCV\n",
    "losocv_loader = LOSOCVSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders, input_dims = losocv_loader.get_data_loaders(DATASETS_PATH)\n",
    "\n",
    "# Load Model Parameters\n",
    "model_config = load_json(SAN_MODEL_CONFIG)\n",
    "model_config = {\n",
    "    **model_config,\n",
    "    'input_dims': input_dims\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "results = []\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'\\nSubject: {subject_id}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    model = ModularModalityFusionNet(**model_config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PyTorchTrainer(model, train_loader, val_loader, SAN_MODEL_CONFIG, device)\n",
    "    trainer.save_path = trainer.save_path.format(fold=f'subject_{subject_id}')\n",
    "    if i == 0:\n",
    "        trainer.print_model_summary()\n",
    "    trained_model_ckpt = trainer.train()\n",
    "    print(f'Model checkpoint saved to: {trained_model_ckpt}\\n')\n",
    "\n",
    "    result = trainer.validate(trained_model_ckpt)\n",
    "    results.append(result)\n",
    "    break\n",
    "\n",
    "# save the results to pkl\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path = f'src/wesad/WESAD/results/san/wrist_results/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/{current_time}/generalized'\n",
    "save_var(results, f'{save_path}/results.pkl', 'Results')\n",
    "copy_json(SAN_MODEL_CONFIG, f'{save_path}/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# results_path = 'src/wesad/WESAD/results/san/wrist_results/60s_5s_10s/2024_06_20_14_55_01/.pkl'\n",
    "results_path = f'{save_path}/results.pkl'\n",
    "results = load_var(results_path)\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Cross Validation Dataloader for Personalization\n",
    "\n",
    "First we must prepare a new dataloader which exclusively contains one subject's personalized data that is used to cross validate the personalized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import PersonalSensorDataLoader\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "SUBJECT_ID = 2\n",
    "personal_loader = PersonalSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "DATASETS_PATH = personal_loader.prepare_datasets(f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s', SUBJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Personalization via Transfer Learning\n",
    "\n",
    "This will then be used to fine tune the model with the new multiheaded attention blocks architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.train import PyTorchTrainer\n",
    "from src.ml_pipeline.models.attention_models import PersonalizedModalityFusionNet, ModularModalityFusionNet\n",
    "from src.ml_pipeline.data_loader import PersonalSensorDataLoader\n",
    "from src.ml_pipeline.utils import get_active_key, load_json, copy_json\n",
    "from src.utils import save_var\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "SUBJECT_ID = 2\n",
    "\n",
    "DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/subject_{SUBJECT_ID}/personal_dataset.pkl'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "SAN_MODEL_PER_CONFIG = 'config_files/model_training/deep/personalized_modality_fusion_net_config.json'\n",
    "MFN_CKPT_PATH = 'src/wesad/WESAD/ckpts/san/wrist_manual_fe/60s_5s_10s/generalized/subject_2/checkpoint_5.pth'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load Dataloaders for LOSOCV\n",
    "personal_loader = PersonalSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders, input_dims = personal_loader.get_data_loaders(DATASETS_PATH)\n",
    "\n",
    "# Load Model Parameters\n",
    "model_config = load_json(SAN_MODEL_PER_CONFIG)\n",
    "model_config = {\n",
    "    **model_config,\n",
    "    'input_dims': input_dims\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "results = []\n",
    "for i, loaders in enumerate(dataloaders):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']    \n",
    "    print(f'\\nFold: {i}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    model = PersonalizedModalityFusionNet(MFN_CKPT_PATH, ModularModalityFusionNet, **model_config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PyTorchTrainer(model, train_loader, val_loader, SAN_MODEL_PER_CONFIG, device)\n",
    "    trainer.save_path = trainer.save_path.format(fold=f'fold_{i}')\n",
    "    if i == 0:\n",
    "        trainer.print_model_summary()\n",
    "    trained_model_ckpt = trainer.train()\n",
    "    print(f'Model saved at: {trained_model_ckpt}')\n",
    "\n",
    "    result = trainer.validate(trained_model_ckpt)\n",
    "    results.append(result)\n",
    "\n",
    "# save the results to pkl\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path = f'src/wesad/WESAD/results/san/wrist_results/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/{current_time}/personalized'\n",
    "save_var(results, f'{save_path}/results.pkl', 'results')\n",
    "copy_json(SAN_MODEL_PER_CONFIG, f'{save_path}/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "results_path = f'src/wesad/WESAD/results/san/wrist_results/60s_5s_10s/2024_06_22_14_20_25/personalized/results.pkl'\n",
    "results = load_var(results_path)\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self and Cross Attention Network\n",
    "\n",
    "This model now integrates cross attention into the self-attention network for inter-modality and intra-modality feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Subject: 2\n",
      "Train: 2589\n",
      "Val: 28\n",
      "\n",
      "----------------------------------------------------------------\n",
      "                  Layer (type)                              Output Shape              Param #\n",
      "================================================================\n",
      "                      Linear-1                                 [32, 128]               10,624\n",
      "                     Dropout-2                             [32, 10, 128]                    0\n",
      "          PositionalEncoding-3                             [32, 10, 128]                    0\n",
      "                      Linear-4                                 [32, 128]                2,048\n",
      "                     Dropout-5                             [32, 10, 128]                    0\n",
      "          PositionalEncoding-6                             [32, 10, 128]                    0\n",
      "                      Linear-7                                 [32, 128]                2,048\n",
      "                     Dropout-8                             [32, 10, 128]                    0\n",
      "          PositionalEncoding-9                             [32, 10, 128]                    0\n",
      "                     Linear-10                                 [32, 128]                  896\n",
      "                    Dropout-11                             [32, 10, 128]                    0\n",
      "         PositionalEncoding-12                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-13               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-14                             [32, 10, 128]                    0\n",
      "                  LayerNorm-15                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-16                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-17               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-18                             [32, 10, 128]                    0\n",
      "                  LayerNorm-19                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-20                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-21               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-22                             [32, 10, 128]                    0\n",
      "                  LayerNorm-23                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-24                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-25               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-26                             [32, 10, 128]                    0\n",
      "                  LayerNorm-27                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-28                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-29               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-30                             [32, 10, 128]                    0\n",
      "                  LayerNorm-31                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-32                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-33               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-34                             [32, 10, 128]                    0\n",
      "                  LayerNorm-35                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-36                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-37               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-38                             [32, 10, 128]                    0\n",
      "                  LayerNorm-39                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-40                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-41               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-42                             [32, 10, 128]                    0\n",
      "                  LayerNorm-43                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-44                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-45               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-46                             [32, 10, 128]                    0\n",
      "                  LayerNorm-47                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-48                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-49               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-50                             [32, 10, 128]                    0\n",
      "                  LayerNorm-51                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-52                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-53               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-54                             [32, 10, 128]                    0\n",
      "                  LayerNorm-55                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-56                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-57               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                    Dropout-58                             [32, 10, 128]                    0\n",
      "                  LayerNorm-59                             [32, 10, 128]                  256\n",
      "        CrossAttentionBlock-60                             [32, 10, 128]                    0\n",
      "         MultiheadAttention-61               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                  LayerNorm-62                             [32, 10, 128]                  256\n",
      "                    Dropout-63                             [32, 10, 128]                    0\n",
      "                     Linear-64                             [32, 10, 128]               16,512\n",
      "                       ReLU-65                             [32, 10, 128]                    0\n",
      "                    Dropout-66                             [32, 10, 128]                    0\n",
      "                     Linear-67                             [32, 10, 128]               16,512\n",
      "    PositionwiseFeedForward-68                             [32, 10, 128]                    0\n",
      "                  LayerNorm-69                             [32, 10, 128]                  256\n",
      "                    Dropout-70                             [32, 10, 128]                    0\n",
      "               EncoderLayer-71                             [32, 10, 128]                    0\n",
      "                     Linear-72                                 [32, 256]               65,792\n",
      "                       ReLU-73                                 [32, 256]                    0\n",
      "                    Dropout-74                                 [32, 256]                    0\n",
      "                     Linear-75                                   [32, 2]                  514\n",
      "         MultiheadAttention-76               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                  LayerNorm-77                             [32, 10, 128]                  256\n",
      "                    Dropout-78                             [32, 10, 128]                    0\n",
      "                     Linear-79                             [32, 10, 128]               16,512\n",
      "                       ReLU-80                             [32, 10, 128]                    0\n",
      "                    Dropout-81                             [32, 10, 128]                    0\n",
      "                     Linear-82                             [32, 10, 128]               16,512\n",
      "    PositionwiseFeedForward-83                             [32, 10, 128]                    0\n",
      "                  LayerNorm-84                             [32, 10, 128]                  256\n",
      "                    Dropout-85                             [32, 10, 128]                    0\n",
      "               EncoderLayer-86                             [32, 10, 128]                    0\n",
      "                     Linear-87                                 [32, 256]               65,792\n",
      "                       ReLU-88                                 [32, 256]                    0\n",
      "                    Dropout-89                                 [32, 256]                    0\n",
      "                     Linear-90                                   [32, 2]                  514\n",
      "         MultiheadAttention-91               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                  LayerNorm-92                             [32, 10, 128]                  256\n",
      "                    Dropout-93                             [32, 10, 128]                    0\n",
      "                     Linear-94                             [32, 10, 128]               16,512\n",
      "                       ReLU-95                             [32, 10, 128]                    0\n",
      "                    Dropout-96                             [32, 10, 128]                    0\n",
      "                     Linear-97                             [32, 10, 128]               16,512\n",
      "    PositionwiseFeedForward-98                             [32, 10, 128]                    0\n",
      "                  LayerNorm-99                             [32, 10, 128]                  256\n",
      "                   Dropout-100                             [32, 10, 128]                    0\n",
      "              EncoderLayer-101                             [32, 10, 128]                    0\n",
      "                    Linear-102                                 [32, 256]               65,792\n",
      "                      ReLU-103                                 [32, 256]                    0\n",
      "                   Dropout-104                                 [32, 256]                    0\n",
      "                    Linear-105                                   [32, 2]                  514\n",
      "        MultiheadAttention-106               [[-1, 10, 128], [-1, 2, 2]]                    0\n",
      "                 LayerNorm-107                             [32, 10, 128]                  256\n",
      "                   Dropout-108                             [32, 10, 128]                    0\n",
      "                    Linear-109                             [32, 10, 128]               16,512\n",
      "                      ReLU-110                             [32, 10, 128]                    0\n",
      "                   Dropout-111                             [32, 10, 128]                    0\n",
      "                    Linear-112                             [32, 10, 128]               16,512\n",
      "   PositionwiseFeedForward-113                             [32, 10, 128]                    0\n",
      "                 LayerNorm-114                             [32, 10, 128]                  256\n",
      "                   Dropout-115                             [32, 10, 128]                    0\n",
      "              EncoderLayer-116                             [32, 10, 128]                    0\n",
      "                    Linear-117                                 [32, 256]               65,792\n",
      "                      ReLU-118                                 [32, 256]                    0\n",
      "                   Dropout-119                                 [32, 256]                    0\n",
      "                    Linear-120                                   [32, 2]                  514\n",
      "                      ReLU-121                                [32, 1024]                    0\n",
      "                   Dropout-122                                [32, 1024]                    0\n",
      "                    Linear-123                                   [32, 2]                2,050\n",
      "================================================================\n",
      "Total params: 420,106\n",
      "Trainable params: 420,106\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 13.51\n",
      "Forward/backward pass size (MB): 28.25\n",
      "Params size (MB): 1.60\n",
      "Estimated Total Size (MB): 43.37\n",
      "----------------------------------------------------------------\n",
      "Storing tensorboard log to: src/wesad/WESAD/ckpts/bcsa/wrist_manual_fe/60s_5s_10s/mmfn/subject_2/tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  65%|██████▌   | 53/81 [00:36<00:19,  1.47it/s, loss=0.117] \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.ml_pipeline.train import PyTorchTrainer\n",
    "from src.ml_pipeline.models.attention_models import ModularBCSA\n",
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "from src.ml_pipeline.utils import load_json, copy_json\n",
    "from src.utils import save_var\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}_mini/wrist_manual_fe_{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}.hdf5'\n",
    "DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/losocv_datasets.pkl'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s_mini/wrist_features.hdf5'\n",
    "BCSA_MODEL_CONFIG = 'config_files/model_training/deep/bsca_config.json'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load Dataloaders for LOSOCV\n",
    "losocv_loader = LOSOCVSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders, input_dims = losocv_loader.get_data_loaders(DATASETS_PATH)\n",
    "\n",
    "# Load Model Parameters\n",
    "model_config = load_json(BCSA_MODEL_CONFIG)\n",
    "model_config = {\n",
    "    **model_config,\n",
    "    'input_dims': input_dims\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "results = []\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'\\nSubject: {subject_id}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    model = ModularBCSA(**model_config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PyTorchTrainer(model, train_loader, val_loader, BCSA_MODEL_CONFIG, device)\n",
    "    trainer.save_path = trainer.save_path.format(fold=f'subject_{subject_id}')\n",
    "    \n",
    "    if i == 0:\n",
    "        trainer.print_model_summary()\n",
    "\n",
    "    trained_model_ckpt = trainer.train()\n",
    "    print(f'Model checkpoint saved to: {trained_model_ckpt}\\n')\n",
    "\n",
    "    result = trainer.validate(trained_model_ckpt)\n",
    "    results.append(result)\n",
    "    break\n",
    "\n",
    "# save the results to pkl\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path = f'src/wesad/WESAD/results/bcsa/wrist_results/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/{current_time}/generalized'\n",
    "save_var(results, f'{save_path}/results.pkl', 'Results')\n",
    "copy_json(BCSA_MODEL_CONFIG, f'{save_path}/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# results_path = f'{save_path}/results.pkl'\n",
    "results_path = 'src/wesad/WESAD/results/bcsa/wrist_results/60s_5s_10s/2024_06_21_15_50_27/generalized/results.pkl'\n",
    "results = load_var(results_path)\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular Autoregressive Co-Attention Network (MARCONet)  \n",
    "\n",
    "This model now integrates modularity to the Self and Cross Attention Network for inter-modality and intra-modality feature extraction using both early fusion of pairwise modalities and late fusion of the ensemble learning branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LOSOCV Datasets on a Per Sensor Basis\n",
    "\n",
    "Now, using the preprocessed `.pkl` files we will make it into a dataloader with LOSOCV (Leave one subject out cross validation) on a per sensor basis. The data augmented samples will be used in the training set but ignored in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "losocv_loader = LOSOCVSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "\n",
    "# Prepare the datasets\n",
    "DATASETS_PATH = losocv_loader.prepare_datasets(f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate Models with LOSOCV\n",
    "\n",
    "Now we can use the prepared datasets and form dataloaders which will then be used to perform LOSOCV on the models. Using the config file we can set the models that we want to test and their corresponding hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.train import PyTorchTrainer\n",
    "from src.ml_pipeline.models.attention_models import MARCONet\n",
    "from src.ml_pipeline.data_loader import LOSOCVSensorDataLoader\n",
    "from src.ml_pipeline.utils import get_active_key, get_key, load_json, copy_json\n",
    "from src.utils import save_var\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}_mini/wrist_manual_fe_{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}.hdf5'\n",
    "\n",
    "DATASETS_PATH = f'src/wesad/WESAD/datasets/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/losocv_datasets.pkl'\n",
    "WRIST_CONFIG = 'config_files/dataset/wesad_wrist_configuration.json'\n",
    "WRIST_FE = f'src/wesad/WESAD/manual_fe/wrist_manual_fe/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/wrist_features.hdf5'\n",
    "SAN_MODEL_CONFIG = 'config_files/model_training/deep/modular_coattention_config.json'\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    # 'num_workers': 4\n",
    "}\n",
    "\n",
    "# Load Dataloaders for LOSOCV\n",
    "losocv_loader = LOSOCVSensorDataLoader(WRIST_FE, WRIST_CONFIG, **dataloader_params)\n",
    "dataloaders, input_dims = losocv_loader.get_data_loaders(DATASETS_PATH)\n",
    "\n",
    "# Load Model Parameters\n",
    "model_config = load_json(SAN_MODEL_CONFIG)\n",
    "model_config = {\n",
    "    **model_config,\n",
    "    'input_dims': input_dims\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "results = []\n",
    "for i, (subject_id, loaders) in enumerate(dataloaders.items()):\n",
    "    train_loader = loaders['train']\n",
    "    val_loader = loaders['val']\n",
    "    \n",
    "    print(f'\\nSubject: {subject_id}')\n",
    "    print(f'Train: {len(train_loader.dataset)}')\n",
    "    print(f'Val: {len(val_loader.dataset)}')\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    model = MARCONet(**model_config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PyTorchTrainer(model, train_loader, val_loader, SAN_MODEL_CONFIG, device)\n",
    "    trainer.save_path = trainer.save_path.format(fold=f'subject_{subject_id}')\n",
    "    if i == 0:\n",
    "        trainer.print_model_summary()\n",
    "    trained_model_ckpt = trainer.train()\n",
    "    print(f'Model checkpoint saved to: {trained_model_ckpt}\\n')\n",
    "\n",
    "    result = trainer.validate(trained_model_ckpt)\n",
    "    results.append(result)\n",
    "    break\n",
    "\n",
    "# save the results to pkl\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path = f'src/wesad/WESAD/results/san/wrist_results/{WINDOW_LENGTH}s_{SLIDING_LENGTH}s_{SPLIT_LENGTH}s/{current_time}/generalized'\n",
    "save_var(results, f'{save_path}/results.pkl', 'Results')\n",
    "copy_json(SAN_MODEL_CONFIG, f'{save_path}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml_pipeline.analysis import ModelResultsAnalysis\n",
    "from src.utils import load_var\n",
    "\n",
    "WINDOW_LENGTH = 60\n",
    "SLIDING_LENGTH = 5\n",
    "SPLIT_LENGTH = 10 # this will split each 60 second segments into 6 x 10 second segments\n",
    "\n",
    "# results_path = 'src/wesad/WESAD/results/san/wrist_results/60s_5s_10s/2024_06_20_14_55_01/.pkl'\n",
    "results_path = f'{save_path}/results.pkl'\n",
    "results = load_var(results_path)\n",
    "\n",
    "analysis = ModelResultsAnalysis(results)\n",
    "analysis.analyze_collective()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Dev_C11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
